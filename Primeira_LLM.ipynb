{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO98Yre8XkLz1rD/1LAGRgg"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FNuPiVBkFAp"
      },
      "outputs": [],
      "source": [
        "#Instalando a biblioteca no Google Gemin\n",
        "!pip install -q -U google-generativeai\n",
        "\n",
        "#Importando bibliotecas do Google Gemin, mumpy e pandas\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import google.generativeai as gemini\n",
        "\n",
        "#Obtendo dados do usuário\n",
        "from google.colab import userdata\n",
        "\n",
        "#Armazenando a chave (API_KEY) em uma variável do colab\n",
        "api_key = userdata.get('SECRET_KEY')\n",
        "\n",
        "#Atribuindo a chave às configurações do gemini\n",
        "gemini.configure(api_key=api_key)\n",
        "\n",
        "for m in gemini.list_models():\n",
        "\tif 'embedContent' in m.supported_generation_methods:\n",
        "\t\tprint(m.name)\n",
        "\n",
        "#Gerando o primeiro Embed (Contexto)\n",
        "text = \"Hello world\"\n",
        "result = gemini.embed_content(model=\"models/embedding-001\", content=text)\n",
        "print(result['embedding'])\n",
        "print(len(result['embedding']))\n",
        "\n",
        "#Gerando o segundo Embed (Contexto)\n",
        "text = [\n",
        "    \"What is the meaning of life?\",\n",
        "    \"How much wood would a woodchuck chuck?\",\n",
        "    \"How does the brain work?\"]\n",
        "result = gemini.embed_content(model=\"models/embedding-001\", content=text)\n",
        "for embedding in result['embedding']:\n",
        "  print(str(embedding)[:50], \"...TRIMMED\")\n",
        "\n",
        "#Gerando o terceiro Embed (Contexto)\n",
        "title = \"A próxima geração de IA para desenvolvedores e Google Workspace\"\n",
        "sample_text = (\"Título: A próxima geração de IA para desenvolvedores e Google Workspace\"\n",
        "    \"\\n\"\n",
        "    \"Artigo completo:\\n\"\n",
        "    \"\\n\"\n",
        "    \"Gemin API & Google AI Studio: Uma maneira acessível de explorar e criar protótipos com aplicações de IA generativas\")\n",
        "\n",
        "embedding = gemini.embed_content(model=\"models/embedding-001\",\n",
        "                                 content=sample_text,\n",
        "                                 title=title,\n",
        "                                 task_type=\"RETRIEVAL_DOCUMENT\")\n",
        "print(embedding)\n",
        "\n",
        "#Listagem de documentos\n",
        "document1 = {\n",
        "    \"Título\": \"Operação do sistema de controle climático\",\n",
        "    \"Conteúdo\": \"O Googlecar tem um sistema de controle climático que permite ajustar\"}\n",
        "\n",
        "document2 = {\n",
        "    \"Título\": \"TouchScreen\",\n",
        "    \"Conteúdo\": \"O seu Googlecar tem uma grande tela sensível ao toque que fornece acesso à internet\"}\n",
        "\n",
        "document3 = {\n",
        "    \"Título\": \"Mudança de marcha\",\n",
        "    \"Conteúdo\": \"O Googlecar tem um transmissão automática. Para trocar as marchas.\"}\n",
        "\n",
        "documents = [document1, document2, document3]\n",
        "\n",
        "df = pd.DataFrame(documents)\n",
        "df.columns = ['Titulo', 'Conteudo']\n",
        "\n",
        "model = \"models/embedding-001\"\n",
        "\n",
        "def embed_fn(title, text):\n",
        "  return gemini.embed_content(model=model, content=text, title=title, task_type=\"RETRIEVAL_DOCUMENT\")[\"embedding\"]\n",
        "\n",
        "df[\"Embeddings\"] = df.apply(lambda row: embed_fn(row[\"Titulo\"], row[\"Conteudo\"]), axis=1)\n",
        "df\n",
        "\n",
        "#Gerando função para retornar conteudo do Embedding criando anteriormente\n",
        "def consultar_documento(consulta, base, model):\n",
        "  embedding = gemini.embed_content(model=model, content=consulta, task_type=\"RETRIEVAL_QUERY\")[\"embedding\"]\n",
        "\n",
        "  produtos_escalares = np.dot(np.stack(df[\"Embeddings\"]), embedding)\n",
        "  indice = np.argmax(produtos_escalares)\n",
        "\n",
        "  return df.iloc[indice][\"Conteudo\"]\n",
        "\n",
        "consulta = \"Como trocar marcha?\"\n",
        "resultado = consultar_documento(consulta, df, model)\n",
        "print(resultado)\n",
        "\n",
        "#Mesclando o modelo criado acima (embaddings) com o modelo existente (gemini)\n",
        "prompt = f\"Reescreva esse texto de uma forma mais criativa: {resultado}\"\n",
        "generation_conf = {\n",
        "    \"temperature\": 0.5,\n",
        "    \"candidate_count\": 1,\n",
        "}\n",
        "model2 = gemini.GenerativeModel(\"gemini-1.0-pro\",\n",
        "                                generation_config=generation_conf)\n",
        "response = model2.generate_content(prompt)\n",
        "print(response.text)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TzqR8-VAoJpy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}